import streamlit as st
import torch
from transformers import AutoModelForImageClassification, AutoImageProcessor
from PIL import Image
from torchvision import transforms
from streamlit_option_menu import option_menu
import os

# Cargar el modelo y el procesador de im√°genes
model_path = '/content/drive/MyDrive/PT/trained_model'
model = AutoModelForImageClassification.from_pretrained(model_path)
image_processor = AutoImageProcessor.from_pretrained(model_path)

# Definir las transformaciones de la imagen
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
])

# Funci√≥n para predecir la emoci√≥n en una imagen
def predict_emotion(image):
    image = transform(image).unsqueeze(0)
    model.eval()
    with torch.no_grad():
        outputs = model(image)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)
        top_probs, top_labels = torch.topk(probs, probs.size(1), dim=1)
    predictions = [{'score': score.item(), 'label': model.config.id2label[label.item()]}
                   for score, label in zip(top_probs[0], top_labels[0])]
    return predictions

# Inicializar el estado de la aplicaci√≥n
if 'menu_option' not in st.session_state:
    st.session_state.menu_option = "Principal"
if 'selected_image' not in st.session_state:
    st.session_state.selected_image = None
if 'uploaded_image' not in st.session_state:
    st.session_state.uploaded_image = None  # Variable para almacenar la imagen cargada

# Interfaz de Streamlit
st.set_page_config(page_title="Detecci√≥n de emociones", layout="centered")

with st.sidebar:
    st.header("Bienvenid@ a mi aplicaci√≥n.")
    st.write("""
    Esta es una aplicaci√≥n para la detecci√≥n de emociones, dise√±ada para identificar las diferentes
    emociones que una persona puede expresar. Las im√°genes no solo capturan momentos, tambi√©n son
    poderosas fuentes de emociones. Un solo gesto puede revelar una historia, y esta herramienta est√°
    aqu√≠ para ayudarte a descifrarla.
    ¬°Explora y descubre las emociones a trav√©s de tus im√°genes, porque a veces una imagen dice m√°s que mil palabras!""")
    st.write("---")
    st.write("")
    st.write("")
    st.write("")
    st.write("Si necesitas ayuda presiona el botonüòÄ")
    if st.sidebar.button("Ayuda"):
      st.sidebar.markdown("Video de ayuda: [Haz clic aqu√≠](https://www.youtube.com/watch?v=zeS2FlxF_0s&t=1702s)")

# Agregar logos en la parte superior
logo1 = Image.open('/content/drive/MyDrive/PT/Imagenes_Streamlit/logo.png')
logo2 = Image.open('/content/drive/MyDrive/PT/Imagenes_Streamlit/UAM.png')

# Contenedor para los logos
logo_container = st.container()
with logo_container:
    col1, col2, col3 = st.columns([1, 2, 1])
    with col1:
        st.image(logo1, width=100)
    with col2:
        st.write("")
    with col3:
        st.image(logo2, width=400)

# Men√∫ de opciones en la parte superior
st.session_state.menu_option = option_menu("___________________Menu___________________",
                                             [ "Conoce", "Principal",  "Cr√©ditos"],
                                             icons=[ 'question-circle','house',  'person-fill'],
                                             menu_icon='three-dots',
                                             default_index=1,
                                             orientation='horizontal')

# Define un diccionario de emociones y sus emojis
emotion_emojis = {
    "Feliz": "üòä",
    "Triste": "üò¢",
    "Sorpresa": "üòÆ",
    "Enojo": "üò°",
    "Miedo": "üò±",
    "Desagrado": "üòñ",
    "Neutral": "üòê"
}

#Opcion de principal y componentes
if st.session_state.menu_option == "Principal":

    st.header("Detecci√≥n de Emociones en Im√°genes")
    st.write("---")
    st.write("üò≤ ¬°Descubre la emoci√≥n que tu imagen puede revelar! üòÑüì∏")
    st.write("""Esta aplicaci√≥n est√° dise√±ada para analizar im√°genes que muestren un solo rostro.
    Aseg√∫rate de que la imagen destaque una √∫nica cara para obtener los mejores resultados.""")

    # Opci√≥n para cargar una imagen desde la PC
    st.subheader("Cargar imagen desde mi PC")
    uploaded_file = st.file_uploader("", type=["jpg", "jpeg", "png"], label_visibility="collapsed")

    col1, col2 = st.columns([2, 1])
    col2.subheader("Tomar una foto")
    camera_photo = col2.camera_input(" ", label_visibility="collapsed")

    # Revisar si hay un archivo subido o una foto tomada
    image = None

    if uploaded_file is not None:
        try:
            image = Image.open(uploaded_file).convert('RGB')
            col2.success("La imagen fue cargada correctamente.")
            st.session_state.uploaded_image = image  # Almacena la imagen cargada
            st.session_state.selected_image = None  # Limpiar selecci√≥n de imagen de prueba
        except Exception:
            col2.error("‚ö†Ô∏èError al cargar la imagen üò•. Por favor aseg√∫rate de que el archivo sea v√°lido üßê.")
    elif camera_photo is not None:
        try:
            image = Image.open(camera_photo).convert('RGB')
            col2.success("La foto fue tomada correctamente.")
            st.session_state.uploaded_image = image  # Almacena la foto tomada
            st.session_state.selected_image = None  # Limpiar selecci√≥n de imagen de prueba
        except Exception:
            col2.error("Error al tomar la foto.")

    # Galer√≠a de im√°genes de prueba
    st.write("### Im√°genes para probar")
    test_images_folder = '/content/drive/MyDrive/PT/Imagenes_Streamlit/Prueba_pagina'
    image_files = [f for f in os.listdir(test_images_folder) if f.endswith(('jpg', 'jpeg', 'png'))]

    num_images_per_row = 5
    num_rows = (len(image_files) + num_images_per_row - 1) // num_images_per_row

    for row in range(num_rows):
        cols = st.columns(num_images_per_row)
        for col in range(num_images_per_row):
            idx = row * num_images_per_row + col
            if idx < len(image_files):
                img_file = image_files[idx]
                img_path = os.path.join(test_images_folder, img_file)
                test_image = Image.open(img_path).convert('RGB')
                with cols[col]:
                    st.image(test_image, width=100)
                    if st.button(f"Probar", key=img_file):
                        st.session_state.selected_image = img_file
                        st.session_state.uploaded_image = None

    # Si hay una imagen disponible (cargada o seleccionada de la galer√≠a), mostrarla a la derecha y las probabilidades a la izquierda
    if image is not None or st.session_state.selected_image is not None:
        image_to_classify = image if image is not None else Image.open(os.path.join(test_images_folder, st.session_state.selected_image)).convert('RGB')
        col2.image(image_to_classify, caption='Imagen Cargada', width=300)
        with col1:
            st.write("Clasificando...")
            predictions = predict_emotion(image_to_classify)

            # Muestra la emoci√≥n detectada con su emoji
            detected_emotion = predictions[0]['label']
            emoji = emotion_emojis.get(detected_emotion, "‚ùì")  # Usa un emoji de pregunta si no se encuentra la emoci√≥n
            st.write("Emoci√≥n detectada:")
            st.write(f"**{detected_emotion} {emoji}**")

            st.write("Probabilidades:")
            for pred in predictions:
                st.progress(pred['score'])
                emoji = emotion_emojis.get(pred['label'], "‚ùì")  # Usa un emoji de pregunta si no se encuentra la emoci√≥n
                st.write(f"{pred['label']} {emoji}: {int(pred['score'] * 100)}%")

# Opci√≥n de conoce y sus componentes
elif st.session_state.menu_option == "Conoce":
    st.header("Transformadores Visuales y Reconocimiento de Emociones")
    st.write("### Transformadores Visuales (Vision Transformers - ViT)")
    st.write("""Los transformadores visuales (ViT) son un avance reciente en el campo de la visi√≥n por computadora.
    Basados en la arquitectura de transformadores utilizada originalmente para procesamiento de lenguaje natural,
    los ViT dividen las im√°genes en 'parches' y procesan estos parches de manera similar a c√≥mo los
    transformadores manejan secuencias de texto. Esto permite que el modelo aprenda relaciones espaciales
    complejas entre diferentes partes de la imagen, resultando en una comprensi√≥n m√°s profunda del contenido.""")
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/conoce1.png',
                 caption='Arquitectura de Vision Transformers',
                 width=400)

    # Segunda secci√≥n: Aplicaciones de los Modelos de Vision Transformers
    st.write("---")
    col1, col2 = st.columns([2, 1])
    with col1:
        st.write("### Aplicaciones de los Modelos de Vision Transformers")
        st.write("""El uso de Vision Transformers en el reconocimiento de emociones es
        particularmente beneficioso, ya que pueden capturar detalles sutiles en las
        expresiones faciales humanas. Esto les permite identificar emociones con mayor
        precisi√≥n en situaciones complejas, como en im√°genes con m√∫ltiples personas o
        con expresiones faciales mezcladas. Su capacidad para aprender patrones complejos
        a partir de grandes cantidades de datos facilita su uso en tareas como el an√°lisis
        de emociones en tiempo real.""")
    with col2:
        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/aplicacion.jpg',
                 caption='ViT en acci√≥n detectando emociones',
                 width=300)

    # Tercera secci√≥n: ViT para el Reconocimiento de Emociones
    st.write("---")
    col1, col2 = st.columns([1, 2])
    with col2:
        st.write("### Modelo ViT para el Reconocimiento de Emociones")
    col1, spacer, col2 = st.columns([1, 0.5, 2])
    with col1:
        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/conoce2.png',
                 caption='Modelo ViT para emociones',
                 width=250)
    with col2:
        st.write("""En nuestra aplicaci√≥n, utilizamos un modelo de Vision Transformers
        entrenado espec√≠ficamente para la clasificaci√≥n de emociones en im√°genes.
        Este modelo ha sido ajustado con un conjunto de datos robusto que incluye
        expresiones faciales diversas. Gracias a su arquitectura, el modelo puede
        distinguir entre emociones como felicidad, tristeza, sorpresa y m√°s con
        alta precisi√≥n, lo que lo convierte en una herramienta valiosa para
        aplicaciones en psicolog√≠a, atenci√≥n al cliente, y m√°s.""")

elif st.session_state.menu_option == "Cr√©ditos":
    st.header("Cr√©ditos")
    st.write("Desarrollado por: Sebastian Hern√°ndez Mej√≠a")
    st.write("Alumno de la carrera Licenciatura en Ingenier√≠a en Computaci√≥n")
    st.write("""Basado en 'Mothercreater/vit-Facial-Expression-Recognition', este modelo fue ajustado finamente con un conjunto de datos
    adicional que incluye im√°genes de rostros en diversas expresiones emocionales.""")
    st.write("Datos: Se utiliz√≥ un conjunto de im√°genes de AffectNet para el entrenamiento.")
    st.write("Agradecimientos especiales a mis asesores:")
    st.write("- Dra. Silvia Beatriz Gonz√°lez Brambila")
    st.write("- M. en C. Josu√© Figueroa Gonz√°lez")
