{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebasHM12/Proyecto_Final/blob/main/PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzyYC1Jbz8G"
      },
      "source": [
        "Librerias y conexión con drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6ckDHt7Ox0J"
      },
      "outputs": [],
      "source": [
        "#Instalar bibliotecas necesarias\n",
        "!pip install numpy matplotlib opencv-python mediapipe\n",
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "import os\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NsfKEUUtply"
      },
      "source": [
        "#Etapa 1\n",
        "#Preprocesamiento de imágenes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzQ6tVF1ujvm"
      },
      "outputs": [],
      "source": [
        "# Definir funciones de procesamiento de imagen\n",
        "# Aplicación de redimención\n",
        "def resize_image(img, size=(224, 224)):\n",
        "    \"\"\"Redimensionar la imagen a un tamaño específico.\"\"\"\n",
        "    return cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
        "# Aplicación de filtro bilateral\n",
        "def bilateral_filter(img, d=7, sigmaColor=31, sigmaSpace=31):\n",
        "    \"\"\"Aplicar un filtro bilateral para suavizar la imagen sin perder detalles.\"\"\"\n",
        "    return cv2.bilateralFilter(img, d, sigmaColor, sigmaSpace)\n",
        "# Aplicaón de contraste\n",
        "def contrast(image, alpha=1.1, beta=2):\n",
        "    \"\"\"Mejorar el contraste de la imagen.\"\"\"\n",
        "    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "\n",
        "# Entradas y salidas\n",
        "input_dirs = [\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Happy\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Sad\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Neutral\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Surprise\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Fear\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Disgust\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Angry\"\n",
        "]\n",
        "output_dir = \"/content/drive/MyDrive/PT/Emotions_Dataset/\"\n",
        "\n",
        "# Asegura que el directorio de salida exista, si no, lo crea\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#The for loop should be at the same indentation level as the previous line\n",
        "for input_dir in input_dirs:\n",
        "    # Obtener el nombre de la carpeta\n",
        "    folder_name = os.path.basename(input_dir)\n",
        "\n",
        "    # Crear el directorio de salida específico para la carpeta\n",
        "    folder_output_dir = os.path.join(output_dir, folder_name)\n",
        "    os.makedirs(folder_output_dir, exist_ok=True)\n",
        "\n",
        "    # Obtener la lista de archivos en el directorio de entrada\n",
        "    files = os.listdir(input_dir)\n",
        "\n",
        "    # Procesar cada archivo de imagen\n",
        "    for idx, file in enumerate(files):\n",
        "        if file.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
        "            # Cargar la imagen\n",
        "            img_path = os.path.join(input_dir, file)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "            if img is None:\n",
        "                continue  # Si no se puede leer la imagen, pasar a la siguiente\n",
        "\n",
        "            # Redimensionar la imagen a 224x224\n",
        "            resized = resize_image(img, size=(224, 224))\n",
        "\n",
        "            # Aplicar filtro bilateral\n",
        "            smoothed = bilateral_filter(resized, d=1,\n",
        "                                        sigmaColor=10, sigmaSpace=75)\n",
        "\n",
        "            # Aplicar contraste\n",
        "            contrasted = contrast(smoothed, alpha=1.2, beta=2)\n",
        "\n",
        "            # Guardar la imagen procesada en el directorio de salida\n",
        "            output_path = os.path.join(folder_output_dir, file)\n",
        "            cv2.imwrite(output_path, contrasted)\n",
        "\n",
        "\n",
        "print(\"Proceso completado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrZQM0mJcsnP"
      },
      "source": [
        "#Estapa 2\n",
        "# Arquitectura y entrenamiento del modelo (ViT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GDqLxErpTC4"
      },
      "outputs": [],
      "source": [
        "# Instalación de la librería 'datasets' para trabajar con conjuntos de datos de Hugging Face\n",
        "!pip install --upgrade datasets\n",
        "# Importación de librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer, AutoConfig\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomRotation,\n",
        "    RandomResizedCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        "    ColorJitter,\n",
        "    RandomPerspective,\n",
        "    RandomAffine,\n",
        "    RandomVerticalFlip,\n",
        "    RandomAdjustSharpness\n",
        "\n",
        ")\n",
        "from transformers import TrainerCallback\n",
        "import itertools\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import drive\n",
        "from torch.optim import AdamW\n",
        "from transformers.optimization import get_scheduler\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cargar conjunto de datos\n",
        "data_dir = '/content/drive/MyDrive/PT/Emotions_Dataset'\n",
        "\n",
        "# Cargue el conjunto de datos usando el formato 'carpeta de imágenes'\n",
        "my_dataset = load_dataset(\"imagefolder\", data_dir=data_dir)\n",
        "\n",
        "# Defina el punto de control del modelo\n",
        "model_checkpoint =\"motheecreator/vit-Facial-Expression-Recognition\"\n",
        "\n",
        "# Cargar el procesador de imágenes asociado al modelo preentrenado\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Obtenga la media, la norma y el tamaño de la imagen para normalizarla y cambiar su tamaño\n",
        "image_mean, image_std = image_processor.image_mean, image_processor.image_std\n",
        "size = image_processor.size[\"height\"]\n",
        "\n",
        "normalize = Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "# Define las transformaciones, para el entrenamiento y evaluación del modelo\n",
        "# Las tranformaciones ayudan al modelo a ser mas robusto y tener más variedad de datos.\n",
        "# Algunas transformaciones que se incluyen son: redimensión, rotaciones, converir a tensor y normalizar.\n",
        "train_tf = Compose(\n",
        "    [\n",
        "        Resize((size, size)),\n",
        "        RandomRotation(90),\n",
        "        RandomAdjustSharpness(2),\n",
        "        RandomHorizontalFlip(0.5),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "# Definir las transformaciones para las imágenes de validación (solo redimensionado y normalización)\n",
        "val_tf = Compose(\n",
        "    [\n",
        "        Resize((size, size)),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Aplicar transformaciones a las imágenes de entrenamiento\n",
        "def train_transforms(examples):\n",
        "    examples['pixel_values'] = [train_tf(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "# Aplicar transformaciones a las imágenes de validación\n",
        "def val_transforms(examples):\n",
        "    examples['pixel_values'] = [val_tf(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento (90%) y validación (10%) usando splits\n",
        "splits = my_dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_data = splits['train']\n",
        "val_data = splits['test']\n",
        "\n",
        "# Aplicar las transformaciones definidas a los conjuntos de entrenamiento y validación\n",
        "train_data.set_transform(train_transforms)\n",
        "val_data.set_transform(val_transforms)\n",
        "\n",
        "# Obtener nombres de las etiquetas del dataset\n",
        "labels = my_dataset[\"train\"].features[\"label\"].names\n",
        "\n",
        "# Crear mapeos de etiquetas de nombres a índices y viceversa\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "# Cargar la configuración del modelo\n",
        "config = AutoConfig.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Configurar el dropout\n",
        "config.hidden_dropout_prob = 0.2  # Porcentaje de Dropout en capas ocultas\n",
        "config.attention_probs_dropout_prob = 0.4  # Porcentaje de Dropout en atención\n",
        "\n",
        "\n",
        "# Cargar el modelo preentrenado para la clasificación de imágenes\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,  # Asignar los mapeos de etiquetas al modelo\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True # Ignorar tamaños de pesos si hay discrepancias\n",
        ")\n",
        "\n",
        "\n",
        "# Crear un callback para almacenar las métricas de entrenamiento y observar como avanza el mismo\n",
        "class TrainingMetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.train_losses = [] # Lista para almacenar las pérdidas de entrenamiento\n",
        "        self.val_losses = [] # Lista para almacenar las pérdidas de validación\n",
        "        self.accuracies = [] # Lista para almacenar las precisiones de validación\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Guardar las métricas de entrenamiento cuando se registran para las curvas de aprendizaje\n",
        "        if 'loss' in logs:\n",
        "            self.train_losses.append(logs['loss'])\n",
        "        if 'eval_loss' in logs:\n",
        "            self.val_losses.append(logs['eval_loss'])\n",
        "        if 'eval_accuracy' in logs:\n",
        "            self.accuracies.append(logs['eval_accuracy'])\n",
        "\n",
        "# Crear una instancia del callback para el entrenamiento\n",
        "training_metrics_callback = TrainingMetricsCallback()\n",
        "\n",
        "# Configurar argumentos de entrenamiento\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./resultados\", # Directorio donde se guardarán los resultados\n",
        "    logging_dir=\"./logs\",      # Directorio donde se guardarán los registros de entrenamiento\n",
        "    remove_unused_columns=False, # Mantener todas las columnas del dataset\n",
        "    eval_strategy=\"steps\",     # Estrategia de evaluación durante el entrenamiento\n",
        "    save_strategy=\"steps\",     # Estrategia de guardado del modelo durante el entrenamiento\n",
        "    learning_rate=3e-05,       # Tasa de aprendizaje para el optimizador\n",
        "    lr_scheduler_type=\"cosine\",  # Tipo de scheduler de tasa de aprendizaje\n",
        "    per_device_train_batch_size=32, # Tamaño de batch para entrenamiento\n",
        "    gradient_accumulation_steps=8,  # Acumulación de gradientes para pasos\n",
        "    per_device_eval_batch_size=32, # Tamaño de batch para evaluación\n",
        "    weight_decay=0.1,              # Decaimiento de peso para el optimizador\n",
        "    num_train_epochs= 16,           # Número de épocas de entrenamiento\n",
        "    warmup_steps=1000,             # Pasos de calentamiento para el scheduler\n",
        "    logging_steps=100,              # Registros de entrenamiento\n",
        "    eval_steps=50,                 # Registros de evaluación\n",
        "    load_best_model_at_end=True,   # Cargar el mejor modelo al finalizar\n",
        "    metric_for_best_model=\"accuracy\", # Métricas para el mejor modelo\n",
        "    push_to_hub=False,             # No subir el modelo a Hugging Face\n",
        "    report_to=\"none\"               # No generar reportes\n",
        ")\n",
        "\n",
        "\n",
        "# Función para calcular las métricas de evaluación\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    labels = p.label_ids\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": accuracy}\n",
        "# Función collate para preparar los datos en batches\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
        "        'labels': torch.tensor([item['label'] for item in batch])\n",
        "    }\n",
        "\n",
        "\n",
        "# Definir el optimizador y el scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.999),\n",
        "                  eps=1e-08)\n",
        "scheduler = get_scheduler(\n",
        "    name=\"cosine\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.warmup_steps,\n",
        "    num_training_steps=args.num_train_epochs * len(train_data)\n",
        ")\n",
        "# Definir el callback para la parada temprana, este ayudara a que el modelo no se sobreajuste\n",
        "# La métrica que vigila es la de eval_accuracy, si esta no mejora despue de 2 veces el entrenamiento se detiene.\n",
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, patience=2, metric=\"eval_accuracy\"):\n",
        "        super().__init__()\n",
        "        self.patience = patience  # Número de evaluaciones consecutivas sin mejora\n",
        "        self.metric = metric\n",
        "        self.best_metric = None\n",
        "        self.num_bad_epochs = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "        current_metric = metrics.get(self.metric)\n",
        "        if self.best_metric is None or current_metric > self.best_metric:\n",
        "            self.best_metric = current_metric\n",
        "            self.num_bad_epochs = 0\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            control.should_training_stop = True  # Indica que el entrenamiento debe detenerse\n",
        "\n",
        "# Crear una instancia del callback para la parada temprana\n",
        "early_stopping_callback = EarlyStoppingCallback(patience=2, metric=\"eval_accuracy\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "trainer = Trainer(\n",
        "    model=model,  # Modelo a entrenar\n",
        "    args=args,    # Argumentos de entrenamiento\n",
        "    train_dataset=train_data,  # Conjunto de datos de entrenamiento\n",
        "    eval_dataset=val_data,     # Conjunto de datos de validación\n",
        "    tokenizer=image_processor,  # Procesador de imágenes\n",
        "    compute_metrics=compute_metrics,  # Función para calcular métricas\n",
        "    data_collator=collate_fn,  # Función para preparar datos en batches\n",
        "    optimizers=(optimizer, scheduler),  # Optimizador y scheduler\n",
        "    callbacks=[training_metrics_callback, early_stopping_callback]  # Añadir ambos callbacks\n",
        ")\n",
        "\n",
        "\n",
        "# Entrenar el modelo\n",
        "trainer.train()\n",
        "\n",
        "# Evaluar el modelo y guardar las métricas\n",
        "metrics = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "# Graficar la curva de aprendizaje después del entrenamiento\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(range(0, len(training_metrics_callback.train_losses)*args.eval_steps, args.eval_steps),\n",
        "         training_metrics_callback.train_losses, label='Training Loss')\n",
        "\n",
        "plt.plot(range(0, len(training_metrics_callback.val_losses)*args.eval_steps, args.eval_steps),\n",
        "         training_metrics_callback.val_losses, label='Validation Loss')\n",
        "\n",
        "plt.plot(range(0, len(training_metrics_callback.accuracies)*args.eval_steps, args.eval_steps),\n",
        "         training_metrics_callback.accuracies, label='Validation Accuracy')\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss/Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Learning Curves')\n",
        "plt.show()\n",
        "\n",
        "# Guardar modelo\n",
        "# Guardar el modelo entrenado y el procesador de imágenes en Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/PT/trained_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Crear el directorio si no existe\n",
        "trainer.save_model(output_dir)  # Guardar el modelo entrenado\n",
        "image_processor.save_pretrained(output_dir)  # Guardar el procesador de imágenes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsTdSKIkq9_M"
      },
      "source": [
        " # Evaluación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lOOP_UrdUNI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor, Trainer\n",
        "import pandas as pd\n",
        "\n",
        "# Definir las rutas\n",
        "dataset_path = \"/content/drive/MyDrive/PT/Evaluacion\"\n",
        "model_path = '/content/drive/MyDrive/PT/trained_model'\n",
        "output_dir = \"/content/evaluation_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Cargar el procesador de imagen y las transformaciones\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # Ajustar al tamaño esperado por el modelo\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)  # Normalización basada en el procesador\n",
        "])\n",
        "\n",
        "# Cargar el conjunto de datos, transformaciones y división del conjunto\n",
        "dataset = ImageFolder(root=dataset_path)\n",
        "dataset.transform = transform\n",
        "test_indices = np.arange(len(dataset))\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "# Crear DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "# Cargar el modelo\n",
        "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
        "# Definir el collator personalizado\n",
        "class CustomCollator:\n",
        "    def __call__(self, batch):\n",
        "        images = torch.stack([item[0] for item in batch])\n",
        "        labels = torch.tensor([item[1] for item in batch])\n",
        "        return {'pixel_values': images, 'labels': labels}\n",
        "\n",
        "data_collator = CustomCollator()\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=image_processor\n",
        ")\n",
        "\n",
        "# Evaluar el modelo\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\")\n",
        "print(eval_results)\n",
        "\n",
        "# Guardar resultados de evaluación\n",
        "eval_results_path = os.path.join(output_dir, \"eval_results.json\")\n",
        "with open(eval_results_path, \"w\") as f:\n",
        "    f.write(str(eval_results))\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Calcular métricas\n",
        "accuracy = accuracy_score(labels, preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Función para graficar la matriz de confusión\n",
        "def plot_confusion_matrix(cm, labels, output_path):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Verdadero')\n",
        "    plt.title('Matriz de Confusión')\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()\n",
        "\n",
        "emotions_labels = ['Enojado', 'Desagrado', 'Miedo', 'Feliz', 'Neutral', 'Triste', 'Sorpresa']\n",
        "\n",
        "# Graficar y guardar la matriz de confusión\n",
        "confusion_matrix_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plot_confusion_matrix(cm, emotions_labels, confusion_matrix_path)\n",
        "\n",
        "# Función para graficar la distribución de clases\n",
        "def plot_class_distribution(labels, preds, class_names, output_path):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    sns.countplot(x=labels, ax=axs[0], palette='viridis')\n",
        "    axs[0].set_title('Distribución de Clases Reales')\n",
        "    axs[0].set_xticklabels(class_names, rotation=90)\n",
        "    axs[0].set_ylabel('Número de Imágenes')\n",
        "    axs[0].set_xlabel('Clases')\n",
        "\n",
        "    sns.countplot(x=preds, ax=axs[1], palette='viridis')\n",
        "    axs[1].set_title('Distribución de Clases Predichas')\n",
        "    axs[1].set_xticklabels(class_names, rotation=90)\n",
        "    axs[1].set_ylabel('Número de Imágenes')\n",
        "    axs[1].set_xlabel('Clases')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()\n",
        "\n",
        "# Guardar gráfico de distribución de clases\n",
        "class_distribution_path = os.path.join(output_dir, \"class_distribution.png\")\n",
        "plot_class_distribution(labels, preds, emotions_labels, class_distribution_path)\n",
        "\n",
        "# Crear y guardar reporte de clasificación\n",
        "classification_report_str = classification_report(labels, preds, target_names=emotions_labels, digits=4)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report_str)\n",
        "\n",
        "classification_report_path = os.path.join(output_dir, \"classification_report.txt\")\n",
        "with open(classification_report_path, \"w\") as f:\n",
        "    f.write(classification_report_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIQWfbw0o6wq"
      },
      "source": [
        "# LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzT-E2AjRvlv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Instalar las dependencias necesarias\n",
        "!pip install lime torch transformers matplotlib\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from PIL import Image\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carga el extractor de características y el modelo\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model.eval()\n",
        "\n",
        "# Carpeta con las imágenes\n",
        "carpeta_imagenes = \"/content/drive/MyDrive/PT/LIME\"\n",
        "\n",
        "# Función para preprocesar una imagen\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
        "    image_array = inputs[\"pixel_values\"].squeeze(0).permute(1, 2, 0).numpy()\n",
        "    return image_array\n",
        "\n",
        "# Define la función de predicción para LIME\n",
        "def predict_fn(images):\n",
        "    model_inputs = torch.tensor(images).permute(0, 3, 1, 2)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(model_inputs)\n",
        "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    return probabilities.numpy()\n",
        "\n",
        "# Inicializa LIME\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Función para analizar y mostrar las imágenes y las explicaciones de LIME\n",
        "def analyze_images(image_folder):\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "    fig, axes = plt.subplots(len(image_files), 2, figsize=(10, 5 * len(image_files)))\n",
        "\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "\n",
        "        image_array = preprocess_image(image_path)\n",
        "\n",
        "        # Explicar la predicción usando LIME\n",
        "        explanation = explainer.explain_instance(\n",
        "            image_array,\n",
        "            predict_fn,\n",
        "            top_labels=5,\n",
        "            hide_color=0,\n",
        "            num_samples=1000,\n",
        "        )\n",
        "\n",
        "        temp, mask = explanation.get_image_and_mask(\n",
        "            label=explanation.top_labels[0],\n",
        "            positive_only=True,\n",
        "            num_features=5,\n",
        "            hide_rest=False\n",
        "        )\n",
        "\n",
        "        # Mostrar la imagen original\n",
        "        axes[i, 0].imshow(image_array / 2 + 0.5)\n",
        "        axes[i, 0].set_title(f\"Imagen Original: {image_file}\")\n",
        "        axes[i, 0].axis(\"off\")\n",
        "\n",
        "        # Mostrar la explicación de LIME\n",
        "        axes[i, 1].imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
        "        axes[i, 1].set_title(f\"Explicación LIME: {image_file}\")\n",
        "        axes[i, 1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "analyze_images(carpeta_imagenes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard\n"
      ],
      "metadata": {
        "id": "kuP2xSDk_WJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from lime import lime_image\n",
        "from PIL import Image\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from sklearn.metrics import jaccard_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carga el extractor de características y el modelo\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model.eval()\n",
        "\n",
        "# Función para preprocesar una imagen\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
        "    image_array = inputs[\"pixel_values\"].squeeze(0).permute(1, 2, 0).numpy()\n",
        "    return image_array\n",
        "\n",
        "# Define la función de predicción para LIME\n",
        "def predict_fn(images):\n",
        "    model_inputs = torch.tensor(images).permute(0, 3, 1, 2)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(model_inputs)\n",
        "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    return probabilities.numpy()\n",
        "\n",
        "# Inicializa LIME\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "image1_path = \"/content/drive/MyDrive/PT/LIME/Sad.jpg\"\n",
        "image2_path = \"/content/drive/MyDrive/PT/LIME/Surprice.jpg\"\n",
        "\n",
        "image1_array = preprocess_image(image1_path)\n",
        "image2_array = preprocess_image(image2_path)\n",
        "\n",
        "# Obtener explicaciones con LIME para ambas imágenes\n",
        "explanation1 = explainer.explain_instance(\n",
        "    image1_array,\n",
        "    predict_fn,\n",
        "    top_labels=5,\n",
        "    hide_color=0,\n",
        "    num_samples=1000\n",
        ")\n",
        "\n",
        "explanation2 = explainer.explain_instance(\n",
        "    image2_array,\n",
        "    predict_fn,\n",
        "    top_labels=5,\n",
        "    hide_color=0,\n",
        "    num_samples=1000\n",
        ")\n",
        "\n",
        "# Obtener las máscaras de ambas imágenes\n",
        "_, mask_image1 = explanation1.get_image_and_mask(\n",
        "    label=explanation1.top_labels[0],\n",
        "    positive_only=True,\n",
        "    num_features=5,\n",
        "    hide_rest=False\n",
        ")\n",
        "\n",
        "_, mask_image2 = explanation2.get_image_and_mask(\n",
        "    label=explanation2.top_labels[0],\n",
        "    positive_only=True,\n",
        "    num_features=5,\n",
        "    hide_rest=False\n",
        ")\n",
        "\n",
        "# Función para calcular el índice de Jaccard\n",
        "def calculate_jaccard(mask1, mask2):\n",
        "    mask1_flat = mask1.flatten()\n",
        "    mask2_flat = mask2.flatten()\n",
        "    return jaccard_score(mask1_flat, mask2_flat, average='binary')\n",
        "\n",
        "# Calcular el índice de Jaccard\n",
        "jaccard_index = calculate_jaccard(mask_image1, mask_image2)\n",
        "\n",
        "print(f\"Índice de Jaccard entre las dos imágenes: {jaccard_index:.4f}\")\n",
        "\n",
        "# Visualizar las imágenes y sus máscaras\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "# Imagen 1\n",
        "axes[0, 0].imshow(image1_array / 2 + 0.5)\n",
        "axes[0, 0].set_title(\"Imagen 1 Original\")\n",
        "axes[0, 1].imshow(mask_image1)\n",
        "axes[0, 1].set_title(\"Máscara LIME Imagen 1\")\n",
        "\n",
        "# Imagen 2\n",
        "axes[1, 0].imshow(image2_array / 2 + 0.5)\n",
        "axes[1, 0].set_title(\"Imagen 2 Original\")\n",
        "axes[1, 1].imshow(mask_image2)\n",
        "axes[1, 1].set_title(\"Máscara LIME Imagen 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qiw0D7VA_VZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTo8XmlsMzQw"
      },
      "source": [
        "# Etapa 3\n",
        "# Aplicacion web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kJSjv8X5MyQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65122305-b228-4e61-d72e-60171408e134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.38.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.38.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Requirement already satisfied: streamlit-option-menu in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: streamlit>=0.63 in /usr/local/lib/python3.10/dist-packages (from streamlit-option-menu) (1.38.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-option-menu) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-option-menu) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-option-menu) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-option-menu) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=0.63->streamlit-option-menu) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Instalar librerias\n",
        "!pip install streamlit\n",
        "!pip install --upgrade streamlit\n",
        "!pip install streamlit-option-menu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from streamlit_option_menu import option_menu\n",
        "import os\n",
        "\n",
        "# Cargar el modelo y el procesador de imágenes\n",
        "model_path = '/content/drive/MyDrive/PT/trained_model'\n",
        "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "# Definir las transformaciones de la imagen\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "])\n",
        "\n",
        "# Función para predecir la emoción en una imagen\n",
        "def predict_emotion(image):\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        top_probs, top_labels = torch.topk(probs, probs.size(1), dim=1)\n",
        "    predictions = [{'score': score.item(), 'label': model.config.id2label[label.item()]}\n",
        "                   for score, label in zip(top_probs[0], top_labels[0])]\n",
        "    return predictions\n",
        "\n",
        "# Inicializar el estado de la aplicación\n",
        "if 'menu_option' not in st.session_state:\n",
        "    st.session_state.menu_option = \"Principal\"\n",
        "if 'selected_image' not in st.session_state:\n",
        "    st.session_state.selected_image = None\n",
        "if 'uploaded_image' not in st.session_state:\n",
        "    st.session_state.uploaded_image = None  # Variable para almacenar la imagen cargada\n",
        "\n",
        "# Interfaz de Streamlit\n",
        "st.set_page_config(page_title=\"Detección de emociones\", layout=\"centered\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Bienvenid@ a mi aplicación.\")\n",
        "    st.write(\"\"\"\n",
        "    Esta es una aplicación para la detección de emociones, diseñada para identificar las diferentes\n",
        "    emociones que una persona puede expresar. Las imágenes no solo capturan momentos, también son\n",
        "    poderosas fuentes de emociones. Un solo gesto puede revelar una historia, y esta herramienta está\n",
        "    aquí para ayudarte a descifrarla.\n",
        "    ¡Explora y descubre las emociones a través de tus imágenes, porque a veces una imagen dice más que mil palabras!\"\"\")\n",
        "    st.write(\"---\")\n",
        "    st.write(\"\")\n",
        "    st.write(\"\")\n",
        "    st.write(\"\")\n",
        "    st.write(\"Si necesitas ayuda presiona el boton😀\")\n",
        "    if st.sidebar.button(\"Ayuda\"):\n",
        "      st.sidebar.markdown(\"Video de ayuda: [Haz clic aquí](https://www.youtube.com/watch?v=zeS2FlxF_0s&t=1702s)\")\n",
        "\n",
        "# Agregar logos en la parte superior\n",
        "logo1 = Image.open('/content/drive/MyDrive/PT/Imagenes_Streamlit/logo.png')\n",
        "logo2 = Image.open('/content/drive/MyDrive/PT/Imagenes_Streamlit/UAM.png')\n",
        "\n",
        "# Contenedor para los logos\n",
        "logo_container = st.container()\n",
        "with logo_container:\n",
        "    col1, col2, col3 = st.columns([1, 2, 1])\n",
        "    with col1:\n",
        "        st.image(logo1, width=100)\n",
        "    with col2:\n",
        "        st.write(\"\")\n",
        "    with col3:\n",
        "        st.image(logo2, width=400)\n",
        "\n",
        "# Menú de opciones en la parte superior\n",
        "st.session_state.menu_option = option_menu(\"___________________Menu___________________\",\n",
        "                                             [ \"Conoce\", \"Principal\",  \"Créditos\"],\n",
        "                                             icons=[ 'question-circle','house',  'person-fill'],\n",
        "                                             menu_icon='three-dots',\n",
        "                                             default_index=1,\n",
        "                                             orientation='horizontal')\n",
        "\n",
        "# Define un diccionario de emociones y sus emojis\n",
        "emotion_emojis = {\n",
        "    \"Feliz\": \"😊\",\n",
        "    \"Triste\": \"😢\",\n",
        "    \"Sorpresa\": \"😮\",\n",
        "    \"Enojo\": \"😡\",\n",
        "    \"Miedo\": \"😱\",\n",
        "    \"Desagrado\": \"😖\",\n",
        "    \"Neutral\": \"😐\"\n",
        "}\n",
        "\n",
        "#Opcion de principal y componentes\n",
        "if st.session_state.menu_option == \"Principal\":\n",
        "\n",
        "    st.header(\"Detección de Emociones en Imágenes\")\n",
        "    st.write(\"---\")\n",
        "    st.write(\"😲 ¡Descubre la emoción que tu imagen puede revelar! 😄📸\")\n",
        "    st.write(\"\"\"Esta aplicación está diseñada para analizar imágenes que muestren un solo rostro.\n",
        "    Asegúrate de que la imagen destaque una única cara para obtener los mejores resultados.\"\"\")\n",
        "\n",
        "    # Opción para cargar una imagen desde la PC\n",
        "    st.subheader(\"Cargar imagen desde mi PC\")\n",
        "    uploaded_file = st.file_uploader(\"\", type=[\"jpg\", \"jpeg\", \"png\"], label_visibility=\"collapsed\")\n",
        "\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "    col2.subheader(\"Tomar una foto\")\n",
        "    camera_photo = col2.camera_input(\" \", label_visibility=\"collapsed\")\n",
        "\n",
        "    # Revisar si hay un archivo subido o una foto tomada\n",
        "    image = None\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            image = Image.open(uploaded_file).convert('RGB')\n",
        "            col2.success(\"La imagen fue cargada correctamente.\")\n",
        "            st.session_state.uploaded_image = image  # Almacena la imagen cargada\n",
        "            st.session_state.selected_image = None  # Limpiar selección de imagen de prueba\n",
        "        except Exception:\n",
        "            col2.error(\"⚠️Error al cargar la imagen 😥. Por favor asegúrate de que el archivo sea válido 🧐.\")\n",
        "    elif camera_photo is not None:\n",
        "        try:\n",
        "            image = Image.open(camera_photo).convert('RGB')\n",
        "            col2.success(\"La foto fue tomada correctamente.\")\n",
        "            st.session_state.uploaded_image = image  # Almacena la foto tomada\n",
        "            st.session_state.selected_image = None  # Limpiar selección de imagen de prueba\n",
        "        except Exception:\n",
        "            col2.error(\"Error al tomar la foto.\")\n",
        "\n",
        "    # Galería de imágenes de prueba\n",
        "    st.write(\"### Imágenes para probar\")\n",
        "    test_images_folder = '/content/drive/MyDrive/PT/Imagenes_Streamlit/Prueba'\n",
        "    image_files = [f for f in os.listdir(test_images_folder) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
        "\n",
        "    num_images_per_row = 5\n",
        "    num_rows = (len(image_files) + num_images_per_row - 1) // num_images_per_row\n",
        "\n",
        "    for row in range(num_rows):\n",
        "        cols = st.columns(num_images_per_row)\n",
        "        for col in range(num_images_per_row):\n",
        "            idx = row * num_images_per_row + col\n",
        "            if idx < len(image_files):\n",
        "                img_file = image_files[idx]\n",
        "                img_path = os.path.join(test_images_folder, img_file)\n",
        "                test_image = Image.open(img_path).convert('RGB')\n",
        "                with cols[col]:\n",
        "                    st.image(test_image, width=100)\n",
        "                    if st.button(f\"Probar\", key=img_file):\n",
        "                        st.session_state.selected_image = img_file\n",
        "                        st.session_state.uploaded_image = None\n",
        "\n",
        "    # Si hay una imagen disponible (cargada o seleccionada de la galería), mostrarla a la derecha y las probabilidades a la izquierda\n",
        "    if image is not None or st.session_state.selected_image is not None:\n",
        "        image_to_classify = image if image is not None else Image.open(os.path.join(test_images_folder, st.session_state.selected_image)).convert('RGB')\n",
        "        col2.image(image_to_classify, caption='Imagen Cargada', width=300)\n",
        "        with col1:\n",
        "            st.write(\"Clasificando...\")\n",
        "            predictions = predict_emotion(image_to_classify)\n",
        "\n",
        "            # Muestra la emoción detectada con su emoji\n",
        "            detected_emotion = predictions[0]['label']\n",
        "            emoji = emotion_emojis.get(detected_emotion, \"❓\")  # Usa un emoji de pregunta si no se encuentra la emoción\n",
        "            st.write(\"Emoción detectada:\")\n",
        "            st.write(f\"**{detected_emotion} {emoji}**\")\n",
        "\n",
        "            st.write(\"Probabilidades:\")\n",
        "            for pred in predictions:\n",
        "                st.progress(pred['score'])\n",
        "                emoji = emotion_emojis.get(pred['label'], \"❓\")  # Usa un emoji de pregunta si no se encuentra la emoción\n",
        "                st.write(f\"{pred['label']} {emoji}: {pred['score']:.2f}\")\n",
        "\n",
        "# Opción de conoce y sus componentes\n",
        "elif st.session_state.menu_option == \"Conoce\":\n",
        "    st.header(\"Transformadores Visuales y Reconocimiento de Emociones\")\n",
        "    st.write(\"### Transformadores Visuales (Vision Transformers - ViT)\")\n",
        "    st.write(\"\"\"Los transformadores visuales (ViT) son un avance reciente en el campo de la visión por computadora.\n",
        "    Basados en la arquitectura de transformadores utilizada originalmente para procesamiento de lenguaje natural,\n",
        "    los ViT dividen las imágenes en 'parches' y procesan estos parches de manera similar a cómo los\n",
        "    transformadores manejan secuencias de texto. Esto permite que el modelo aprenda relaciones espaciales\n",
        "    complejas entre diferentes partes de la imagen, resultando en una comprensión más profunda del contenido.\"\"\")\n",
        "    col1, col2, col3 = st.columns([1, 2, 1])\n",
        "    with col2:\n",
        "        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/conoce1.png',\n",
        "                 caption='Arquitectura de Vision Transformers',\n",
        "                 width=400)\n",
        "\n",
        "    # Segunda sección: Aplicaciones de los Modelos de Vision Transformers\n",
        "    st.write(\"---\")\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "    with col1:\n",
        "        st.write(\"### Aplicaciones de los Modelos de Vision Transformers\")\n",
        "        st.write(\"\"\"El uso de Vision Transformers en el reconocimiento de emociones es\n",
        "        particularmente beneficioso, ya que pueden capturar detalles sutiles en las\n",
        "        expresiones faciales humanas. Esto les permite identificar emociones con mayor\n",
        "        precisión en situaciones complejas, como en imágenes con múltiples personas o\n",
        "        con expresiones faciales mezcladas. Su capacidad para aprender patrones complejos\n",
        "        a partir de grandes cantidades de datos facilita su uso en tareas como el análisis\n",
        "        de emociones en tiempo real.\"\"\")\n",
        "    with col2:\n",
        "        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/aplicacion.jpg',\n",
        "                 caption='ViT en acción detectando emociones',\n",
        "                 width=300)\n",
        "\n",
        "    # Tercera sección: ViT para el Reconocimiento de Emociones\n",
        "    st.write(\"---\")\n",
        "    col1, col2 = st.columns([1, 2])\n",
        "    with col2:\n",
        "        st.write(\"### Modelo ViT para el Reconocimiento de Emociones\")\n",
        "    col1, spacer, col2 = st.columns([1, 0.5, 2])\n",
        "    with col1:\n",
        "        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/conoce2.png',\n",
        "                 caption='Modelo ViT para emociones',\n",
        "                 width=250)\n",
        "    with col2:\n",
        "        st.write(\"\"\"En nuestra aplicación, utilizamos un modelo de Vision Transformers\n",
        "        entrenado específicamente para la clasificación de emociones en imágenes.\n",
        "        Este modelo ha sido ajustado con un conjunto de datos robusto que incluye\n",
        "        expresiones faciales diversas. Gracias a su arquitectura, el modelo puede\n",
        "        distinguir entre emociones como felicidad, tristeza, sorpresa y más con\n",
        "        alta precisión, lo que lo convierte en una herramienta valiosa para\n",
        "        aplicaciones en psicología, atención al cliente, y más.\"\"\")\n",
        "\n",
        "elif st.session_state.menu_option == \"Créditos\":\n",
        "    st.header(\"Créditos\")\n",
        "    st.write(\"Desarrollado por: Sebastian Hernández Mejía\")\n",
        "    st.write(\"Alumno de la carrera Licenciatura en Ingeniería en Computación\")\n",
        "    st.write(\"\"\"Basado en 'Mothercreater/vit-Facial-Expression-Recognition', este modelo fue ajustado finamente con un conjunto de datos\n",
        "    adicional que incluye imágenes de rostros en diversas expresiones emocionales.\"\"\")\n",
        "    st.write(\"Datos: Se utilizó un conjunto de imágenes de AffectNet para el entrenamiento.\")\n",
        "    st.write(\"Agradecimientos especiales a mis asesores:\")\n",
        "    st.write(\"- Dra. Silvia Beatriz González Brambila\")\n",
        "    st.write(\"- M. en C. Josué Figueroa González\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Z65Lc0Zyy8",
        "outputId": "69742e32-3541-4e86-dae4-49015d7f14c8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmvGNDO3l_3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38ec261-d00b-43c1-f355-8a868463e611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.48.211.185:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://young-keys-refuse.loca.lt\n",
            "2024-09-23 03:23:13.548489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-23 03:23:13.577675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-23 03:23:13.586683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-23 03:23:13.607546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-23 03:23:14.970289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-09-23 03:23:17.874 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:28:36.130 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:31:55.511 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:32:02.630 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:32:10.733 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:32:14.614 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Levantar la aplicación\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tpk89z1GzI1o",
        "m9PZwKC3ZPcj",
        "ryBih1HNv_pZ",
        "MrZQM0mJcsnP",
        "DGe3wDAjcA1G",
        "sIQWfbw0o6wq",
        "bIJeCWi3Tu-O"
      ],
      "provenance": [],
      "mount_file_id": "1I8d4oR9XfJDyx2Np4mSlneB1DrKqDqLp",
      "authorship_tag": "ABX9TyN8qHxCjRHYI2S2c28yap1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}