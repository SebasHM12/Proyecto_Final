{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebasHM12/Proyecto_Final/blob/main/PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzyYC1Jbz8G"
      },
      "source": [
        "Librerias y conexi칩n con drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6ckDHt7Ox0J"
      },
      "outputs": [],
      "source": [
        "#Instalar bibliotecas necesarias\n",
        "!pip install numpy matplotlib opencv-python mediapipe\n",
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "import os\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab.patches import cv2_imshow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NsfKEUUtply"
      },
      "source": [
        "#Etapa 1\n",
        "#Preprocesamiento de im치genes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzQ6tVF1ujvm"
      },
      "outputs": [],
      "source": [
        "# Definir funciones de procesamiento de imagen\n",
        "# Aplicaci칩n de redimenci칩n\n",
        "def resize_image(img, size=(224, 224)):\n",
        "    \"\"\"Redimensionar la imagen a un tama침o espec칤fico.\"\"\"\n",
        "    return cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
        "# Aplicaci칩n de filtro bilateral\n",
        "def bilateral_filter(img, d=7, sigmaColor=31, sigmaSpace=31):\n",
        "    \"\"\"Aplicar un filtro bilateral para suavizar la imagen sin perder detalles.\"\"\"\n",
        "    return cv2.bilateralFilter(img, d, sigmaColor, sigmaSpace)\n",
        "# Aplica칩n de contraste\n",
        "def contrast(image, alpha=1.1, beta=2):\n",
        "    \"\"\"Mejorar el contraste de la imagen.\"\"\"\n",
        "    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "\n",
        "# Entradas y salidas\n",
        "input_dirs = [\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Happy\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Sad\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Neutral\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Surprise\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Fear\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Disgust\",\n",
        "    \"/content/drive/MyDrive/PT/Emotions/Angry\"\n",
        "]\n",
        "output_dir = \"/content/drive/MyDrive/PT/Emotions_Dataset/\"\n",
        "\n",
        "# Asegura que el directorio de salida exista, si no, lo crea\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#The for loop should be at the same indentation level as the previous line\n",
        "for input_dir in input_dirs:\n",
        "    # Obtener el nombre de la carpeta\n",
        "    folder_name = os.path.basename(input_dir)\n",
        "\n",
        "    # Crear el directorio de salida espec칤fico para la carpeta\n",
        "    folder_output_dir = os.path.join(output_dir, folder_name)\n",
        "    os.makedirs(folder_output_dir, exist_ok=True)\n",
        "\n",
        "    # Obtener la lista de archivos en el directorio de entrada\n",
        "    files = os.listdir(input_dir)\n",
        "\n",
        "    # Procesar cada archivo de imagen\n",
        "    for idx, file in enumerate(files):\n",
        "        if file.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
        "            # Cargar la imagen\n",
        "            img_path = os.path.join(input_dir, file)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "            if img is None:\n",
        "                continue  # Si no se puede leer la imagen, pasar a la siguiente\n",
        "\n",
        "            # Redimensionar la imagen a 224x224\n",
        "            resized = resize_image(img, size=(224, 224))\n",
        "\n",
        "            # Aplicar filtro bilateral\n",
        "            smoothed = bilateral_filter(resized, d=1,\n",
        "                                        sigmaColor=10, sigmaSpace=75)\n",
        "\n",
        "            # Aplicar contraste\n",
        "            contrasted = contrast(smoothed, alpha=1.2, beta=2)\n",
        "\n",
        "            # Guardar la imagen procesada en el directorio de salida\n",
        "            output_path = os.path.join(folder_output_dir, file)\n",
        "            cv2.imwrite(output_path, contrasted)\n",
        "\n",
        "\n",
        "print(\"Proceso completado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrZQM0mJcsnP"
      },
      "source": [
        "#Estapa 2\n",
        "# Arquitectura y entrenamiento del modelo (ViT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GDqLxErpTC4"
      },
      "outputs": [],
      "source": [
        "# Instalaci칩n de la librer칤a 'datasets' para trabajar con conjuntos de datos de Hugging Face\n",
        "!pip install --upgrade datasets\n",
        "# Importaci칩n de librer칤as\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer, AutoConfig\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomRotation,\n",
        "    RandomResizedCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        "    ColorJitter,\n",
        "    RandomPerspective,\n",
        "    RandomAffine,\n",
        "    RandomVerticalFlip,\n",
        "    RandomAdjustSharpness\n",
        "\n",
        ")\n",
        "from transformers import TrainerCallback\n",
        "import itertools\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import drive\n",
        "from torch.optim import AdamW\n",
        "from transformers.optimization import get_scheduler\n",
        "\n",
        "# Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cargar conjunto de datos\n",
        "data_dir = '/content/drive/MyDrive/PT/Emotions_Dataset'\n",
        "\n",
        "# Cargue el conjunto de datos usando el formato 'carpeta de im치genes'\n",
        "my_dataset = load_dataset(\"imagefolder\", data_dir=data_dir)\n",
        "\n",
        "# Defina el punto de control del modelo\n",
        "model_checkpoint =\"motheecreator/vit-Facial-Expression-Recognition\"\n",
        "\n",
        "# Cargar el procesador de im치genes asociado al modelo preentrenado\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Obtenga la media, la norma y el tama침o de la imagen para normalizarla y cambiar su tama침o\n",
        "image_mean, image_std = image_processor.image_mean, image_processor.image_std\n",
        "size = image_processor.size[\"height\"]\n",
        "\n",
        "normalize = Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "# Define las transformaciones, para el entrenamiento y evaluaci칩n del modelo\n",
        "# Las tranformaciones ayudan al modelo a ser mas robusto y tener m치s variedad de datos.\n",
        "# Algunas transformaciones que se incluyen son: redimensi칩n, rotaciones, converir a tensor y normalizar.\n",
        "train_tf = Compose(\n",
        "    [\n",
        "        Resize((size, size)),\n",
        "        RandomRotation(90),\n",
        "        RandomAdjustSharpness(2),\n",
        "        RandomHorizontalFlip(0.5),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "# Definir las transformaciones para las im치genes de validaci칩n (solo redimensionado y normalizaci칩n)\n",
        "val_tf = Compose(\n",
        "    [\n",
        "        Resize((size, size)),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Aplicar transformaciones a las im치genes de entrenamiento\n",
        "def train_transforms(examples):\n",
        "    examples['pixel_values'] = [train_tf(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "# Aplicar transformaciones a las im치genes de validaci칩n\n",
        "def val_transforms(examples):\n",
        "    examples['pixel_values'] = [val_tf(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento (90%) y validaci칩n (10%) usando splits\n",
        "splits = my_dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_data = splits['train']\n",
        "val_data = splits['test']\n",
        "\n",
        "# Aplicar las transformaciones definidas a los conjuntos de entrenamiento y validaci칩n\n",
        "train_data.set_transform(train_transforms)\n",
        "val_data.set_transform(val_transforms)\n",
        "\n",
        "# Obtener nombres de las etiquetas del dataset\n",
        "labels = my_dataset[\"train\"].features[\"label\"].names\n",
        "\n",
        "# Crear mapeos de etiquetas de nombres a 칤ndices y viceversa\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "# Cargar la configuraci칩n del modelo\n",
        "config = AutoConfig.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Configurar el dropout\n",
        "config.hidden_dropout_prob = 0.2  # Porcentaje de Dropout en capas ocultas\n",
        "config.attention_probs_dropout_prob = 0.4  # Porcentaje de Dropout en atenci칩n\n",
        "\n",
        "\n",
        "# Cargar el modelo preentrenado para la clasificaci칩n de im치genes\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,  # Asignar los mapeos de etiquetas al modelo\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True # Ignorar tama침os de pesos si hay discrepancias\n",
        ")\n",
        "\n",
        "\n",
        "# Crear un callback para almacenar las m칠tricas de entrenamiento y observar como avanza el mismo\n",
        "class TrainingMetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.train_losses = [] # Lista para almacenar las p칠rdidas de entrenamiento\n",
        "        self.val_losses = [] # Lista para almacenar las p칠rdidas de validaci칩n\n",
        "        self.accuracies = [] # Lista para almacenar las precisiones de validaci칩n\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Guardar las m칠tricas de entrenamiento cuando se registran para las curvas de aprendizaje\n",
        "        if 'loss' in logs:\n",
        "            self.train_losses.append(logs['loss'])\n",
        "        if 'eval_loss' in logs:\n",
        "            self.val_losses.append(logs['eval_loss'])\n",
        "        if 'eval_accuracy' in logs:\n",
        "            self.accuracies.append(logs['eval_accuracy'])\n",
        "\n",
        "# Crear una instancia del callback para el entrenamiento\n",
        "training_metrics_callback = TrainingMetricsCallback()\n",
        "\n",
        "# Configurar argumentos de entrenamiento\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./resultados\", # Directorio donde se guardar치n los resultados\n",
        "    logging_dir=\"./logs\",      # Directorio donde se guardar치n los registros de entrenamiento\n",
        "    remove_unused_columns=False, # Mantener todas las columnas del dataset\n",
        "    eval_strategy=\"steps\",     # Estrategia de evaluaci칩n durante el entrenamiento\n",
        "    save_strategy=\"steps\",     # Estrategia de guardado del modelo durante el entrenamiento\n",
        "    learning_rate=3e-05,       # Tasa de aprendizaje para el optimizador\n",
        "    lr_scheduler_type=\"cosine\",  # Tipo de scheduler de tasa de aprendizaje\n",
        "    per_device_train_batch_size=32, # Tama침o de batch para entrenamiento\n",
        "    gradient_accumulation_steps=8,  # Acumulaci칩n de gradientes para pasos\n",
        "    per_device_eval_batch_size=32, # Tama침o de batch para evaluaci칩n\n",
        "    weight_decay=0.1,              # Decaimiento de peso para el optimizador\n",
        "    num_train_epochs= 16,           # N칰mero de 칠pocas de entrenamiento\n",
        "    warmup_steps=1000,             # Pasos de calentamiento para el scheduler\n",
        "    logging_steps=100,              # Registros de entrenamiento\n",
        "    eval_steps=50,                 # Registros de evaluaci칩n\n",
        "    load_best_model_at_end=True,   # Cargar el mejor modelo al finalizar\n",
        "    metric_for_best_model=\"accuracy\", # M칠tricas para el mejor modelo\n",
        "    push_to_hub=False,             # No subir el modelo a Hugging Face\n",
        "    report_to=\"none\"               # No generar reportes\n",
        ")\n",
        "\n",
        "\n",
        "# Funci칩n para calcular las m칠tricas de evaluaci칩n\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    labels = p.label_ids\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": accuracy}\n",
        "# Funci칩n collate para preparar los datos en batches\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
        "        'labels': torch.tensor([item['label'] for item in batch])\n",
        "    }\n",
        "\n",
        "\n",
        "# Definir el optimizador y el scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=args.learning_rate, betas=(0.9, 0.999),\n",
        "                  eps=1e-08)\n",
        "scheduler = get_scheduler(\n",
        "    name=\"cosine\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=args.warmup_steps,\n",
        "    num_training_steps=args.num_train_epochs * len(train_data)\n",
        ")\n",
        "# Definir el callback para la parada temprana, este ayudara a que el modelo no se sobreajuste\n",
        "# La m칠trica que vigila es la de eval_accuracy, si esta no mejora despue de 2 veces el entrenamiento se detiene.\n",
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, patience=2, metric=\"eval_accuracy\"):\n",
        "        super().__init__()\n",
        "        self.patience = patience  # N칰mero de evaluaciones consecutivas sin mejora\n",
        "        self.metric = metric\n",
        "        self.best_metric = None\n",
        "        self.num_bad_epochs = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "        current_metric = metrics.get(self.metric)\n",
        "        if self.best_metric is None or current_metric > self.best_metric:\n",
        "            self.best_metric = current_metric\n",
        "            self.num_bad_epochs = 0\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            control.should_training_stop = True  # Indica que el entrenamiento debe detenerse\n",
        "\n",
        "# Crear una instancia del callback para la parada temprana\n",
        "early_stopping_callback = EarlyStoppingCallback(patience=2, metric=\"eval_accuracy\")\n",
        "\n",
        "# Entrenar el modelo\n",
        "trainer = Trainer(\n",
        "    model=model,  # Modelo a entrenar\n",
        "    args=args,    # Argumentos de entrenamiento\n",
        "    train_dataset=train_data,  # Conjunto de datos de entrenamiento\n",
        "    eval_dataset=val_data,     # Conjunto de datos de validaci칩n\n",
        "    tokenizer=image_processor,  # Procesador de im치genes\n",
        "    compute_metrics=compute_metrics,  # Funci칩n para calcular m칠tricas\n",
        "    data_collator=collate_fn,  # Funci칩n para preparar datos en batches\n",
        "    optimizers=(optimizer, scheduler),  # Optimizador y scheduler\n",
        "    callbacks=[training_metrics_callback, early_stopping_callback]  # A침adir ambos callbacks\n",
        ")\n",
        "\n",
        "\n",
        "# Entrenar el modelo\n",
        "trainer.train()\n",
        "\n",
        "# Evaluar el modelo y guardar las m칠tricas\n",
        "metrics = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "# Graficar la curva de aprendizaje despu칠s del entrenamiento\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(range(0, len(training_metrics_callback.train_losses)*args.eval_steps, args.eval_steps),\n",
        "         training_metrics_callback.train_losses, label='Training Loss')\n",
        "\n",
        "plt.plot(range(0, len(training_metrics_callback.val_losses)*args.eval_steps, args.eval_steps),\n",
        "         training_metrics_callback.val_losses, label='Validation Loss')\n",
        "\n",
        "plt.plot(range(0, len(training_metrics_callback.accuracies)*args.eval_steps, args.eval_steps),\n",
        "         training_metrics_callback.accuracies, label='Validation Accuracy')\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss/Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Learning Curves')\n",
        "plt.show()\n",
        "\n",
        "# Guardar modelo\n",
        "# Guardar el modelo entrenado y el procesador de im치genes en Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/PT/trained_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Crear el directorio si no existe\n",
        "trainer.save_model(output_dir)  # Guardar el modelo entrenado\n",
        "image_processor.save_pretrained(output_dir)  # Guardar el procesador de im치genes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsTdSKIkq9_M"
      },
      "source": [
        " # Evaluaci칩n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lOOP_UrdUNI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor, Trainer\n",
        "import pandas as pd\n",
        "\n",
        "# Definir las rutas\n",
        "dataset_path = \"/content/drive/MyDrive/PT/Evaluacion\"\n",
        "model_path = '/content/drive/MyDrive/PT/trained_model'\n",
        "output_dir = \"/content/evaluation_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Cargar el procesador de imagen y las transformaciones\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # Ajustar al tama침o esperado por el modelo\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)  # Normalizaci칩n basada en el procesador\n",
        "])\n",
        "\n",
        "# Cargar el conjunto de datos, transformaciones y divisi칩n del conjunto\n",
        "dataset = ImageFolder(root=dataset_path)\n",
        "dataset.transform = transform\n",
        "test_indices = np.arange(len(dataset))\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "# Crear DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "# Cargar el modelo\n",
        "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
        "# Definir el collator personalizado\n",
        "class CustomCollator:\n",
        "    def __call__(self, batch):\n",
        "        images = torch.stack([item[0] for item in batch])\n",
        "        labels = torch.tensor([item[1] for item in batch])\n",
        "        return {'pixel_values': images, 'labels': labels}\n",
        "\n",
        "data_collator = CustomCollator()\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=image_processor\n",
        ")\n",
        "\n",
        "# Evaluar el modelo\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\")\n",
        "print(eval_results)\n",
        "\n",
        "# Guardar resultados de evaluaci칩n\n",
        "eval_results_path = os.path.join(output_dir, \"eval_results.json\")\n",
        "with open(eval_results_path, \"w\") as f:\n",
        "    f.write(str(eval_results))\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = np.argmax(predictions.predictions, axis=1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Calcular m칠tricas\n",
        "accuracy = accuracy_score(labels, preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Funci칩n para graficar la matriz de confusi칩n\n",
        "def plot_confusion_matrix(cm, labels, output_path):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicci칩n')\n",
        "    plt.ylabel('Verdadero')\n",
        "    plt.title('Matriz de Confusi칩n')\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()\n",
        "\n",
        "emotions_labels = ['Enojado', 'Desagrado', 'Miedo', 'Feliz', 'Neutral', 'Triste', 'Sorpresa']\n",
        "\n",
        "# Graficar y guardar la matriz de confusi칩n\n",
        "confusion_matrix_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plot_confusion_matrix(cm, emotions_labels, confusion_matrix_path)\n",
        "\n",
        "# Funci칩n para graficar la distribuci칩n de clases\n",
        "def plot_class_distribution(labels, preds, class_names, output_path):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    sns.countplot(x=labels, ax=axs[0], palette='viridis')\n",
        "    axs[0].set_title('Distribuci칩n de Clases Reales')\n",
        "    axs[0].set_xticklabels(class_names, rotation=90)\n",
        "    axs[0].set_ylabel('N칰mero de Im치genes')\n",
        "    axs[0].set_xlabel('Clases')\n",
        "\n",
        "    sns.countplot(x=preds, ax=axs[1], palette='viridis')\n",
        "    axs[1].set_title('Distribuci칩n de Clases Predichas')\n",
        "    axs[1].set_xticklabels(class_names, rotation=90)\n",
        "    axs[1].set_ylabel('N칰mero de Im치genes')\n",
        "    axs[1].set_xlabel('Clases')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path)\n",
        "    plt.show()\n",
        "\n",
        "# Guardar gr치fico de distribuci칩n de clases\n",
        "class_distribution_path = os.path.join(output_dir, \"class_distribution.png\")\n",
        "plot_class_distribution(labels, preds, emotions_labels, class_distribution_path)\n",
        "\n",
        "# Crear y guardar reporte de clasificaci칩n\n",
        "classification_report_str = classification_report(labels, preds, target_names=emotions_labels, digits=4)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report_str)\n",
        "\n",
        "classification_report_path = os.path.join(output_dir, \"classification_report.txt\")\n",
        "with open(classification_report_path, \"w\") as f:\n",
        "    f.write(classification_report_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIQWfbw0o6wq"
      },
      "source": [
        "# LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzT-E2AjRvlv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Instalar las dependencias necesarias\n",
        "!pip install lime torch transformers matplotlib\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from PIL import Image\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carga el extractor de caracter칤sticas y el modelo\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model.eval()\n",
        "\n",
        "# Carpeta con las im치genes\n",
        "carpeta_imagenes = \"/content/drive/MyDrive/PT/LIME\"\n",
        "\n",
        "# Funci칩n para preprocesar una imagen\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
        "    image_array = inputs[\"pixel_values\"].squeeze(0).permute(1, 2, 0).numpy()\n",
        "    return image_array\n",
        "\n",
        "# Define la funci칩n de predicci칩n para LIME\n",
        "def predict_fn(images):\n",
        "    model_inputs = torch.tensor(images).permute(0, 3, 1, 2)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(model_inputs)\n",
        "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    return probabilities.numpy()\n",
        "\n",
        "# Inicializa LIME\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Funci칩n para analizar y mostrar las im치genes y las explicaciones de LIME\n",
        "def analyze_images(image_folder):\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "    fig, axes = plt.subplots(len(image_files), 2, figsize=(10, 5 * len(image_files)))\n",
        "\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "\n",
        "        image_array = preprocess_image(image_path)\n",
        "\n",
        "        # Explicar la predicci칩n usando LIME\n",
        "        explanation = explainer.explain_instance(\n",
        "            image_array,\n",
        "            predict_fn,\n",
        "            top_labels=5,\n",
        "            hide_color=0,\n",
        "            num_samples=1000,\n",
        "        )\n",
        "\n",
        "        temp, mask = explanation.get_image_and_mask(\n",
        "            label=explanation.top_labels[0],\n",
        "            positive_only=True,\n",
        "            num_features=5,\n",
        "            hide_rest=False\n",
        "        )\n",
        "\n",
        "        # Mostrar la imagen original\n",
        "        axes[i, 0].imshow(image_array / 2 + 0.5)\n",
        "        axes[i, 0].set_title(f\"Imagen Original: {image_file}\")\n",
        "        axes[i, 0].axis(\"off\")\n",
        "\n",
        "        # Mostrar la explicaci칩n de LIME\n",
        "        axes[i, 1].imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
        "        axes[i, 1].set_title(f\"Explicaci칩n LIME: {image_file}\")\n",
        "        axes[i, 1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "analyze_images(carpeta_imagenes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard\n"
      ],
      "metadata": {
        "id": "kuP2xSDk_WJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from lime import lime_image\n",
        "from PIL import Image\n",
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from sklearn.metrics import jaccard_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carga el extractor de caracter칤sticas y el modelo\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"/content/drive/MyDrive/PT/trained_model\")\n",
        "model.eval()\n",
        "\n",
        "# Funci칩n para preprocesar una imagen\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
        "    image_array = inputs[\"pixel_values\"].squeeze(0).permute(1, 2, 0).numpy()\n",
        "    return image_array\n",
        "\n",
        "# Define la funci칩n de predicci칩n para LIME\n",
        "def predict_fn(images):\n",
        "    model_inputs = torch.tensor(images).permute(0, 3, 1, 2)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(model_inputs)\n",
        "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    return probabilities.numpy()\n",
        "\n",
        "# Inicializa LIME\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "image1_path = \"/content/drive/MyDrive/PT/LIME/Sad.jpg\"\n",
        "image2_path = \"/content/drive/MyDrive/PT/LIME/Surprice.jpg\"\n",
        "\n",
        "image1_array = preprocess_image(image1_path)\n",
        "image2_array = preprocess_image(image2_path)\n",
        "\n",
        "# Obtener explicaciones con LIME para ambas im치genes\n",
        "explanation1 = explainer.explain_instance(\n",
        "    image1_array,\n",
        "    predict_fn,\n",
        "    top_labels=5,\n",
        "    hide_color=0,\n",
        "    num_samples=1000\n",
        ")\n",
        "\n",
        "explanation2 = explainer.explain_instance(\n",
        "    image2_array,\n",
        "    predict_fn,\n",
        "    top_labels=5,\n",
        "    hide_color=0,\n",
        "    num_samples=1000\n",
        ")\n",
        "\n",
        "# Obtener las m치scaras de ambas im치genes\n",
        "_, mask_image1 = explanation1.get_image_and_mask(\n",
        "    label=explanation1.top_labels[0],\n",
        "    positive_only=True,\n",
        "    num_features=5,\n",
        "    hide_rest=False\n",
        ")\n",
        "\n",
        "_, mask_image2 = explanation2.get_image_and_mask(\n",
        "    label=explanation2.top_labels[0],\n",
        "    positive_only=True,\n",
        "    num_features=5,\n",
        "    hide_rest=False\n",
        ")\n",
        "\n",
        "# Funci칩n para calcular el 칤ndice de Jaccard\n",
        "def calculate_jaccard(mask1, mask2):\n",
        "    mask1_flat = mask1.flatten()\n",
        "    mask2_flat = mask2.flatten()\n",
        "    return jaccard_score(mask1_flat, mask2_flat, average='binary')\n",
        "\n",
        "# Calcular el 칤ndice de Jaccard\n",
        "jaccard_index = calculate_jaccard(mask_image1, mask_image2)\n",
        "\n",
        "print(f\"칈ndice de Jaccard entre las dos im치genes: {jaccard_index:.4f}\")\n",
        "\n",
        "# Visualizar las im치genes y sus m치scaras\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "# Imagen 1\n",
        "axes[0, 0].imshow(image1_array / 2 + 0.5)\n",
        "axes[0, 0].set_title(\"Imagen 1 Original\")\n",
        "axes[0, 1].imshow(mask_image1)\n",
        "axes[0, 1].set_title(\"M치scara LIME Imagen 1\")\n",
        "\n",
        "# Imagen 2\n",
        "axes[1, 0].imshow(image2_array / 2 + 0.5)\n",
        "axes[1, 0].set_title(\"Imagen 2 Original\")\n",
        "axes[1, 1].imshow(mask_image2)\n",
        "axes[1, 1].set_title(\"M치scara LIME Imagen 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qiw0D7VA_VZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTo8XmlsMzQw"
      },
      "source": [
        "# Etapa 3\n",
        "# Aplicacion web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJSjv8X5MyQD"
      },
      "outputs": [],
      "source": [
        "# Instalar librerias\n",
        "!pip install streamlit\n",
        "!pip install --upgrade streamlit\n",
        "!pip install streamlit-option-menu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from streamlit_option_menu import option_menu\n",
        "import os\n",
        "\n",
        "# Cargar el modelo y el procesador de im치genes\n",
        "model_path = '/content/drive/MyDrive/PT/trained_model'\n",
        "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "# Definir las transformaciones de la imagen\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "])\n",
        "\n",
        "# Funci칩n para predecir la emoci칩n en una imagen\n",
        "def predict_emotion(image):\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        top_probs, top_labels = torch.topk(probs, probs.size(1), dim=1)\n",
        "    predictions = [{'score': score.item(), 'label': model.config.id2label[label.item()]}\n",
        "                   for score, label in zip(top_probs[0], top_labels[0])]\n",
        "    return predictions\n",
        "\n",
        "# Inicializar el estado de la aplicaci칩n\n",
        "if 'menu_option' not in st.session_state:\n",
        "    st.session_state.menu_option = \"Principal\"\n",
        "if 'selected_image' not in st.session_state:\n",
        "    st.session_state.selected_image = None\n",
        "if 'uploaded_image' not in st.session_state:\n",
        "    st.session_state.uploaded_image = None  # Variable para almacenar la imagen cargada\n",
        "\n",
        "# Interfaz de Streamlit\n",
        "st.set_page_config(page_title=\"Detecci칩n de emociones\", layout=\"centered\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Bienvenid@ a mi aplicaci칩n.\")\n",
        "    st.write(\"\"\"\n",
        "    Esta es una aplicaci칩n para la detecci칩n de emociones, dise침ada para identificar las diferentes\n",
        "    emociones que una persona puede expresar. Las im치genes no solo capturan momentos, tambi칠n son\n",
        "    poderosas fuentes de emociones. Un solo gesto puede revelar una historia, y esta herramienta est치\n",
        "    aqu칤 para ayudarte a descifrarla.\n",
        "    춰Explora y descubre las emociones a trav칠s de tus im치genes, porque a veces una imagen dice m치s que mil palabras!\"\"\")\n",
        "    st.write(\"---\")\n",
        "    st.write(\"\")\n",
        "    st.write(\"\")\n",
        "    st.write(\"\")\n",
        "    st.write(\"Si necesitas ayuda presiona el boton游\")\n",
        "    if st.sidebar.button(\"Ayuda\"):\n",
        "      st.sidebar.markdown(\"Video de ayuda: [Haz clic aqu칤](https://www.youtube.com/watch?v=zeS2FlxF_0s&t=1702s)\")\n",
        "\n",
        "# Agregar logos en la parte superior\n",
        "logo1 = Image.open('/content/drive/MyDrive/PT/Imagenes_Streamlit/logo.png')\n",
        "logo2 = Image.open('/content/drive/MyDrive/PT/Imagenes_Streamlit/UAM.png')\n",
        "\n",
        "# Contenedor para los logos\n",
        "logo_container = st.container()\n",
        "with logo_container:\n",
        "    col1, col2, col3 = st.columns([1, 2, 1])\n",
        "    with col1:\n",
        "        st.image(logo1, width=100)\n",
        "    with col2:\n",
        "        st.write(\"\")\n",
        "    with col3:\n",
        "        st.image(logo2, width=400)\n",
        "\n",
        "# Men칰 de opciones en la parte superior\n",
        "st.session_state.menu_option = option_menu(\"___________________Menu___________________\",\n",
        "                                             [ \"Conoce\", \"Principal\",  \"Cr칠ditos\"],\n",
        "                                             icons=[ 'question-circle','house',  'person-fill'],\n",
        "                                             menu_icon='three-dots',\n",
        "                                             default_index=1,\n",
        "                                             orientation='horizontal')\n",
        "\n",
        "# Define un diccionario de emociones y sus emojis\n",
        "emotion_emojis = {\n",
        "    \"Feliz\": \"游땕\",\n",
        "    \"Triste\": \"游땩\",\n",
        "    \"Sorpresa\": \"游땵\",\n",
        "    \"Enojo\": \"游땨\",\n",
        "    \"Miedo\": \"游땸\",\n",
        "    \"Desagrado\": \"游땠\",\n",
        "    \"Neutral\": \"游땛\"\n",
        "}\n",
        "\n",
        "#Opcion de principal y componentes\n",
        "if st.session_state.menu_option == \"Principal\":\n",
        "\n",
        "    st.header(\"Detecci칩n de Emociones en Im치genes\")\n",
        "    st.write(\"---\")\n",
        "    st.write(\"游 춰Descubre la emoci칩n que tu imagen puede revelar! 游땏游닞\")\n",
        "    st.write(\"\"\"Esta aplicaci칩n est치 dise침ada para analizar im치genes que muestren un solo rostro.\n",
        "    Aseg칰rate de que la imagen destaque una 칰nica cara para obtener los mejores resultados.\"\"\")\n",
        "\n",
        "    # Opci칩n para cargar una imagen desde la PC\n",
        "    st.subheader(\"Cargar imagen desde mi PC\")\n",
        "    uploaded_file = st.file_uploader(\"\", type=[\"jpg\", \"jpeg\", \"png\"], label_visibility=\"collapsed\")\n",
        "\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "    col2.subheader(\"Tomar una foto\")\n",
        "    camera_photo = col2.camera_input(\" \", label_visibility=\"collapsed\")\n",
        "\n",
        "    # Revisar si hay un archivo subido o una foto tomada\n",
        "    image = None\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            image = Image.open(uploaded_file).convert('RGB')\n",
        "            col2.success(\"La imagen fue cargada correctamente.\")\n",
        "            st.session_state.uploaded_image = image  # Almacena la imagen cargada\n",
        "            st.session_state.selected_image = None  # Limpiar selecci칩n de imagen de prueba\n",
        "        except Exception:\n",
        "            col2.error(\"丘멆잺Error al cargar la imagen 游땬. Por favor aseg칰rate de que el archivo sea v치lido 游븷.\")\n",
        "    elif camera_photo is not None:\n",
        "        try:\n",
        "            image = Image.open(camera_photo).convert('RGB')\n",
        "            col2.success(\"La foto fue tomada correctamente.\")\n",
        "            st.session_state.uploaded_image = image  # Almacena la foto tomada\n",
        "            st.session_state.selected_image = None  # Limpiar selecci칩n de imagen de prueba\n",
        "        except Exception:\n",
        "            col2.error(\"Error al tomar la foto.\")\n",
        "\n",
        "    # Galer칤a de im치genes de prueba\n",
        "    st.write(\"### Im치genes para probar\")\n",
        "    test_images_folder = '/content/drive/MyDrive/PT/Imagenes_Streamlit/Prueba'\n",
        "    image_files = [f for f in os.listdir(test_images_folder) if f.endswith(('jpg', 'jpeg', 'png'))]\n",
        "\n",
        "    num_images_per_row = 5\n",
        "    num_rows = (len(image_files) + num_images_per_row - 1) // num_images_per_row\n",
        "\n",
        "    for row in range(num_rows):\n",
        "        cols = st.columns(num_images_per_row)\n",
        "        for col in range(num_images_per_row):\n",
        "            idx = row * num_images_per_row + col\n",
        "            if idx < len(image_files):\n",
        "                img_file = image_files[idx]\n",
        "                img_path = os.path.join(test_images_folder, img_file)\n",
        "                test_image = Image.open(img_path).convert('RGB')\n",
        "                with cols[col]:\n",
        "                    st.image(test_image, width=100)\n",
        "                    if st.button(f\"Probar\", key=img_file):\n",
        "                        st.session_state.selected_image = img_file\n",
        "                        st.session_state.uploaded_image = None\n",
        "\n",
        "    # Si hay una imagen disponible (cargada o seleccionada de la galer칤a), mostrarla a la derecha y las probabilidades a la izquierda\n",
        "    if image is not None or st.session_state.selected_image is not None:\n",
        "        image_to_classify = image if image is not None else Image.open(os.path.join(test_images_folder, st.session_state.selected_image)).convert('RGB')\n",
        "        col2.image(image_to_classify, caption='Imagen Cargada', width=300)\n",
        "        with col1:\n",
        "            st.write(\"Clasificando...\")\n",
        "            predictions = predict_emotion(image_to_classify)\n",
        "\n",
        "            # Muestra la emoci칩n detectada con su emoji\n",
        "            detected_emotion = predictions[0]['label']\n",
        "            emoji = emotion_emojis.get(detected_emotion, \"仇늎")  # Usa un emoji de pregunta si no se encuentra la emoci칩n\n",
        "            st.write(\"Emoci칩n detectada:\")\n",
        "            st.write(f\"**{detected_emotion} {emoji}**\")\n",
        "\n",
        "            st.write(\"Probabilidades:\")\n",
        "            for pred in predictions:\n",
        "                st.progress(pred['score'])\n",
        "                emoji = emotion_emojis.get(pred['label'], \"仇늎")  # Usa un emoji de pregunta si no se encuentra la emoci칩n\n",
        "                st.write(f\"{pred['label']} {emoji}: {pred['score']:.2f}\")\n",
        "\n",
        "# Opci칩n de conoce y sus componentes\n",
        "elif st.session_state.menu_option == \"Conoce\":\n",
        "    st.header(\"Transformadores Visuales y Reconocimiento de Emociones\")\n",
        "    st.write(\"### Transformadores Visuales (Vision Transformers - ViT)\")\n",
        "    st.write(\"\"\"Los transformadores visuales (ViT) son un avance reciente en el campo de la visi칩n por computadora.\n",
        "    Basados en la arquitectura de transformadores utilizada originalmente para procesamiento de lenguaje natural,\n",
        "    los ViT dividen las im치genes en 'parches' y procesan estos parches de manera similar a c칩mo los\n",
        "    transformadores manejan secuencias de texto. Esto permite que el modelo aprenda relaciones espaciales\n",
        "    complejas entre diferentes partes de la imagen, resultando en una comprensi칩n m치s profunda del contenido.\"\"\")\n",
        "    col1, col2, col3 = st.columns([1, 2, 1])\n",
        "    with col2:\n",
        "        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/conoce1.png',\n",
        "                 caption='Arquitectura de Vision Transformers',\n",
        "                 width=400)\n",
        "\n",
        "    # Segunda secci칩n: Aplicaciones de los Modelos de Vision Transformers\n",
        "    st.write(\"---\")\n",
        "    col1, col2 = st.columns([2, 1])\n",
        "    with col1:\n",
        "        st.write(\"### Aplicaciones de los Modelos de Vision Transformers\")\n",
        "        st.write(\"\"\"El uso de Vision Transformers en el reconocimiento de emociones es\n",
        "        particularmente beneficioso, ya que pueden capturar detalles sutiles en las\n",
        "        expresiones faciales humanas. Esto les permite identificar emociones con mayor\n",
        "        precisi칩n en situaciones complejas, como en im치genes con m칰ltiples personas o\n",
        "        con expresiones faciales mezcladas. Su capacidad para aprender patrones complejos\n",
        "        a partir de grandes cantidades de datos facilita su uso en tareas como el an치lisis\n",
        "        de emociones en tiempo real.\"\"\")\n",
        "    with col2:\n",
        "        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/aplicacion.jpg',\n",
        "                 caption='ViT en acci칩n detectando emociones',\n",
        "                 width=300)\n",
        "\n",
        "    # Tercera secci칩n: ViT para el Reconocimiento de Emociones\n",
        "    st.write(\"---\")\n",
        "    col1, col2 = st.columns([1, 2])\n",
        "    with col2:\n",
        "        st.write(\"### Modelo ViT para el Reconocimiento de Emociones\")\n",
        "    col1, spacer, col2 = st.columns([1, 0.5, 2])\n",
        "    with col1:\n",
        "        st.image('/content/drive/MyDrive/PT/Imagenes_Streamlit/conoce2.png',\n",
        "                 caption='Modelo ViT para emociones',\n",
        "                 width=250)\n",
        "    with col2:\n",
        "        st.write(\"\"\"En nuestra aplicaci칩n, utilizamos un modelo de Vision Transformers\n",
        "        entrenado espec칤ficamente para la clasificaci칩n de emociones en im치genes.\n",
        "        Este modelo ha sido ajustado con un conjunto de datos robusto que incluye\n",
        "        expresiones faciales diversas. Gracias a su arquitectura, el modelo puede\n",
        "        distinguir entre emociones como felicidad, tristeza, sorpresa y m치s con\n",
        "        alta precisi칩n, lo que lo convierte en una herramienta valiosa para\n",
        "        aplicaciones en psicolog칤a, atenci칩n al cliente, y m치s.\"\"\")\n",
        "\n",
        "elif st.session_state.menu_option == \"Cr칠ditos\":\n",
        "    st.header(\"Cr칠ditos\")\n",
        "    st.write(\"Desarrollado por: Sebastian Hern치ndez Mej칤a\")\n",
        "    st.write(\"Alumno de la carrera Licenciatura en Ingenier칤a en Computaci칩n\")\n",
        "    st.write(\"\"\"Basado en 'Mothercreater/vit-Facial-Expression-Recognition', este modelo fue ajustado finamente con un conjunto de datos\n",
        "    adicional que incluye im치genes de rostros en diversas expresiones emocionales.\"\"\")\n",
        "    st.write(\"Datos: Se utiliz칩 un conjunto de im치genes de AffectNet para el entrenamiento.\")\n",
        "    st.write(\"Agradecimientos especiales a mis asesores:\")\n",
        "    st.write(\"- Dra. Silvia Beatriz Gonz치lez Brambila\")\n",
        "    st.write(\"- M. en C. Josu칠 Figueroa Gonz치lez\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Z65Lc0Zyy8",
        "outputId": "95f37169-8cc3-4e29-abbc-507c81827afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmvGNDO3l_3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38ec261-d00b-43c1-f355-8a868463e611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.48.211.185:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://young-keys-refuse.loca.lt\n",
            "2024-09-23 03:23:13.548489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-23 03:23:13.577675: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-23 03:23:13.586683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-23 03:23:13.607546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-23 03:23:14.970289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-09-23 03:23:17.874 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:28:36.130 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:31:55.511 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:32:02.630 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:32:10.733 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2024-09-23 03:32:14.614 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Levantar la aplicaci칩n\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tpk89z1GzI1o",
        "m9PZwKC3ZPcj",
        "ryBih1HNv_pZ",
        "MrZQM0mJcsnP",
        "DGe3wDAjcA1G",
        "sIQWfbw0o6wq",
        "bIJeCWi3Tu-O"
      ],
      "provenance": [],
      "mount_file_id": "1I8d4oR9XfJDyx2Np4mSlneB1DrKqDqLp",
      "authorship_tag": "ABX9TyN8qHxCjRHYI2S2c28yap1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}